{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4d3a6f308a716b7346984f65bb3d7b67a7d315d6"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "28f6e00a91aa646d1c2bd6f6c39c77f6160a7b84",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, cv2, skimage\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Conv2D, Dense, Dropout, Flatten, Input, Add, GlobalAveragePooling2D, DepthwiseConv2D, BatchNormalization, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "batch_size = 64\n",
    "imageSize = 64\n",
    "target_dims = (imageSize, imageSize, 3)\n",
    "num_classes = 30\n",
    "\n",
    "train_len = 87000\n",
    "train_dir = \"D:/PROJECTS/Github Clones/Conv-now-data/asl_alphabet_train/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(folder):\n",
    "    \"\"\"\n",
    "    Load the data and labels from the given folder.\n",
    "    \"\"\"\n",
    "    #Create feature matrix and labels...\n",
    "    X = np.empty((train_len, imageSize, imageSize, 3), dtype=np.float32)\n",
    "    y = np.empty((train_len,1), dtype=np.int)\n",
    "    cnt = 0\n",
    "    \n",
    "    #Iterate over all folders, give numerical label to each folder...\n",
    "    for folderName in os.listdir(folder):\n",
    "        if not folderName.startswith('.'):\n",
    "            if folderName in ['A']:\n",
    "                label = 0\n",
    "            elif folderName in ['B']:\n",
    "                label = 1\n",
    "            elif folderName in ['C']:\n",
    "                label = 2\n",
    "            elif folderName in ['D']:\n",
    "                label = 3\n",
    "            elif folderName in ['E']:\n",
    "                label = 4\n",
    "            elif folderName in ['F']:\n",
    "                label = 5\n",
    "            elif folderName in ['G']:\n",
    "                label = 6\n",
    "            elif folderName in ['H']:\n",
    "                label = 7\n",
    "            elif folderName in ['I']:\n",
    "                label = 8\n",
    "            elif folderName in ['J']:\n",
    "                label = 9\n",
    "            elif folderName in ['K']:\n",
    "                label = 10\n",
    "            elif folderName in ['L']:\n",
    "                label = 11\n",
    "            elif folderName in ['M']:\n",
    "                label = 12\n",
    "            elif folderName in ['N']:\n",
    "                label = 13\n",
    "            elif folderName in ['O']:\n",
    "                label = 14\n",
    "            elif folderName in ['P']:\n",
    "                label = 15\n",
    "            elif folderName in ['Q']:\n",
    "                label = 16\n",
    "            elif folderName in ['R']:\n",
    "                label = 17\n",
    "            elif folderName in ['S']:\n",
    "                label = 18\n",
    "            elif folderName in ['T']:\n",
    "                label = 19\n",
    "            elif folderName in ['U']:\n",
    "                label = 20\n",
    "            elif folderName in ['V']:\n",
    "                label = 21\n",
    "            elif folderName in ['W']:\n",
    "                label = 22\n",
    "            elif folderName in ['X']:\n",
    "                label = 23\n",
    "            elif folderName in ['Y']:\n",
    "                label = 24\n",
    "            elif folderName in ['Z']:\n",
    "                label = 25\n",
    "            elif folderName in ['del']:\n",
    "                label = 26\n",
    "            elif folderName in ['nothing']:\n",
    "                label = 27\n",
    "            elif folderName in ['space']:\n",
    "                label = 28           \n",
    "            else:\n",
    "                label=29\n",
    "            \n",
    "            \n",
    "            #Now, with label ready, iterate over all images in the folder,\n",
    "            #create a numpy array of the image, add it to X\n",
    "            #and add corressponding label to y...\n",
    "            \n",
    "            for image_filename in os.listdir(folder + folderName):\n",
    "                img_file = cv2.imread(folder + folderName + '/' + image_filename)\n",
    "                if img_file is not None:\n",
    "                    img_file = skimage.transform.resize(img_file, (imageSize, imageSize, 3))\n",
    "                    img_arr = np.asarray(img_file).reshape((-1, imageSize, imageSize, 3))\n",
    "                    \n",
    "                    X[cnt] = img_arr\n",
    "                    y[cnt] = label\n",
    "                    cnt += 1\n",
    "#                     X.append(img_arr)\n",
    "#                     y.append(label)\n",
    "#     X = np.asarray(X)\n",
    "#     y = np.asarray(y)\n",
    "    return X,y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78300, 64, 64, 3) (78300, 1) (8700, 64, 64, 3) (8700, 1)\n",
      "Number of bad labels = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((78300, 64, 64, 3), (78300, 30), (8700, 64, 64, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = get_data(train_dir) \n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.1) \n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "cnt=0\n",
    "for label in y_train:    \n",
    "    if label > 29:\n",
    "        cnt+=1\n",
    "        #print(label)\n",
    "print(f\"Number of bad labels = {cnt}\")\n",
    "        \n",
    "        \n",
    "# Encode labels to hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\n",
    "y_trainHot = to_categorical(y_train, num_classes=num_classes)\n",
    "y_testHot = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "\n",
    "\n",
    "X_train.shape, y_trainHot.shape, X_test.shape#, y_testHot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c596b3cf8e2c68d6e62adf5277da7a1ca9077c6b"
   },
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "932cc8db83f4e84ac816d56110ac28508b8c3c97"
   },
   "outputs": [],
   "source": [
    "train_image_generator = ImageDataGenerator(\n",
    "    samplewise_center=True,\n",
    "    samplewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "val_image_generator = ImageDataGenerator(\n",
    "    samplewise_center=True,\n",
    "    samplewise_std_normalization=True,\n",
    ")\n",
    "\n",
    "train_generator = train_image_generator.flow(x=X_train, y=y_trainHot, batch_size=batch_size, shuffle=True)\n",
    "val_generator = val_image_generator.flow(x=X_test, y=y_testHot, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9801290b12bff9ca77ef29d860681e413f94497c"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "dd77e290cff58996acbbd4f6f75a1c70de37dfc7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\envML\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\envML\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 32)   896         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 64, 64, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 32)   9248        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 64, 64, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 32)   9248        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 32, 32, 32)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   9248        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 32, 32, 32)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 32, 32, 32)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 32)   9248        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 16, 16, 32)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_1 (DepthwiseCo (None, 16, 16, 32)   320         leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 16, 16, 32)   128         depthwise_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 16, 16, 32)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 32)   1056        leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 16, 32)   128         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 16, 16, 32)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_2 (DepthwiseCo (None, 16, 16, 32)   320         leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 16, 32)   128         depthwise_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 16, 16, 32)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   1056        leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 16, 16, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 16, 16, 32)   0           leaky_re_lu_10[0][0]             \n",
      "                                                                 leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 32)           0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32)           0           global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          4224        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 30)           3870        dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 58,494\n",
      "Trainable params: 58,238\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=target_dims)\n",
    "net = Conv2D(32, kernel_size=3, strides=1, padding=\"same\")(inputs)\n",
    "net = LeakyReLU()(net)\n",
    "net = Conv2D(32, kernel_size=3, strides=1, padding=\"same\")(net)\n",
    "net = LeakyReLU()(net)\n",
    "net = Conv2D(32, kernel_size=3, strides=2, padding=\"same\")(net)\n",
    "net = LeakyReLU()(net)\n",
    "\n",
    "net = Conv2D(32, kernel_size=3, strides=1, padding=\"same\")(net)\n",
    "net = LeakyReLU()(net)\n",
    "net = Conv2D(32, kernel_size=3, strides=1, padding=\"same\")(net)\n",
    "net = LeakyReLU()(net)\n",
    "net = Conv2D(32, kernel_size=3, strides=2, padding=\"same\")(net)\n",
    "net = LeakyReLU()(net)\n",
    "\n",
    "shortcut = net\n",
    "\n",
    "net = DepthwiseConv2D(kernel_size=3, strides=1, padding='same', kernel_initializer='he_normal')(net)\n",
    "net = BatchNormalization(axis=3)(net)\n",
    "net = LeakyReLU()(net)\n",
    "net = Conv2D(filters=32, kernel_size=1, strides=1, padding='same', kernel_initializer='he_normal')(net)\n",
    "net = BatchNormalization(axis=3)(net)\n",
    "net = LeakyReLU()(net)\n",
    "\n",
    "net = DepthwiseConv2D(kernel_size=3, strides=1, padding='same', kernel_initializer='he_normal')(net)\n",
    "net = BatchNormalization(axis=3)(net)\n",
    "net = LeakyReLU()(net)\n",
    "net = Conv2D(filters=32, kernel_size=1, strides=1, padding='same', kernel_initializer='he_normal')(net)\n",
    "net = BatchNormalization(axis=3)(net)\n",
    "net = LeakyReLU()(net)\n",
    "\n",
    "net = Add()([net, shortcut])\n",
    "\n",
    "net = GlobalAveragePooling2D()(net)\n",
    "net = Dropout(0.2)(net)\n",
    "\n",
    "net = Dense(128, activation='relu')(net)\n",
    "outputs = Dense(num_classes, activation='softmax')(net)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "16fa58a2542584cc8597564daa8446a04aa92560"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "b03bce0453b2ae35df51edce38921f74e33c0a95",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\envML\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/30\n",
      "1224/1224 [==============================] - 104s 85ms/step - loss: 1.9432 - acc: 0.3842 - val_loss: 0.7881 - val_acc: 0.7137\n",
      "Epoch 2/30\n",
      "1224/1224 [==============================] - 91s 74ms/step - loss: 0.8451 - acc: 0.7079 - val_loss: 0.6487 - val_acc: 0.7606\n",
      "Epoch 3/30\n",
      "1224/1224 [==============================] - 94s 77ms/step - loss: 0.5899 - acc: 0.7977 - val_loss: 0.4582 - val_acc: 0.8341\n",
      "Epoch 4/30\n",
      "1224/1224 [==============================] - 98s 80ms/step - loss: 0.4664 - acc: 0.8411 - val_loss: 0.3418 - val_acc: 0.8805\n",
      "Epoch 5/30\n",
      "1224/1224 [==============================] - 99s 81ms/step - loss: 0.3820 - acc: 0.8703 - val_loss: 0.3569 - val_acc: 0.8763\n",
      "Epoch 6/30\n",
      "1224/1224 [==============================] - 101s 82ms/step - loss: 0.3297 - acc: 0.8893 - val_loss: 0.1048 - val_acc: 0.9678\n",
      "Epoch 7/30\n",
      "1224/1224 [==============================] - 102s 83ms/step - loss: 0.2853 - acc: 0.9044 - val_loss: 0.0914 - val_acc: 0.9674\n",
      "Epoch 8/30\n",
      "1224/1224 [==============================] - 97s 80ms/step - loss: 0.2647 - acc: 0.9114 - val_loss: 0.2101 - val_acc: 0.9297\n",
      "Epoch 9/30\n",
      "1224/1224 [==============================] - 96s 79ms/step - loss: 0.2421 - acc: 0.9197 - val_loss: 0.0531 - val_acc: 0.9839\n",
      "Epoch 10/30\n",
      "1224/1224 [==============================] - 96s 79ms/step - loss: 0.2242 - acc: 0.9261 - val_loss: 0.0717 - val_acc: 0.9766\n",
      "Epoch 11/30\n",
      "1224/1224 [==============================] - 96s 78ms/step - loss: 0.2177 - acc: 0.9269 - val_loss: 0.2445 - val_acc: 0.9254\n",
      "Epoch 12/30\n",
      "1224/1224 [==============================] - 97s 79ms/step - loss: 0.2008 - acc: 0.9336 - val_loss: 0.0915 - val_acc: 0.9686\n",
      "Epoch 13/30\n",
      "1224/1224 [==============================] - 96s 79ms/step - loss: 0.1874 - acc: 0.9388 - val_loss: 0.0736 - val_acc: 0.9741\n",
      "Epoch 14/30\n",
      "1224/1224 [==============================] - 96s 79ms/step - loss: 0.1774 - acc: 0.9416 - val_loss: 0.0582 - val_acc: 0.9799\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 15/30\n",
      "1224/1224 [==============================] - 96s 78ms/step - loss: 0.0921 - acc: 0.9712 - val_loss: 0.0139 - val_acc: 0.9961\n",
      "Epoch 16/30\n",
      "1224/1224 [==============================] - 97s 79ms/step - loss: 0.0836 - acc: 0.9732 - val_loss: 0.0125 - val_acc: 0.9966\n",
      "Epoch 17/30\n",
      "1224/1224 [==============================] - 97s 79ms/step - loss: 0.0837 - acc: 0.9733 - val_loss: 0.0119 - val_acc: 0.9966\n",
      "Epoch 18/30\n",
      "1224/1224 [==============================] - 98s 80ms/step - loss: 0.0796 - acc: 0.9746 - val_loss: 0.0153 - val_acc: 0.9949\n",
      "Epoch 19/30\n",
      "1224/1224 [==============================] - 97s 79ms/step - loss: 0.0770 - acc: 0.9752 - val_loss: 0.0142 - val_acc: 0.9951\n",
      "Epoch 20/30\n",
      "1224/1224 [==============================] - 98s 80ms/step - loss: 0.0737 - acc: 0.9760 - val_loss: 0.0124 - val_acc: 0.9962\n",
      "Epoch 21/30\n",
      "1224/1224 [==============================] - 99s 81ms/step - loss: 0.0724 - acc: 0.9764 - val_loss: 0.0140 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 22/30\n",
      "1224/1224 [==============================] - 98s 80ms/step - loss: 0.0592 - acc: 0.9820 - val_loss: 0.0091 - val_acc: 0.9971\n",
      "Epoch 23/30\n",
      "1224/1224 [==============================] - 102s 84ms/step - loss: 0.0575 - acc: 0.9815 - val_loss: 0.0084 - val_acc: 0.9972\n",
      "Epoch 24/30\n",
      "1224/1224 [==============================] - 103s 84ms/step - loss: 0.0573 - acc: 0.9814 - val_loss: 0.0084 - val_acc: 0.9975\n",
      "Epoch 25/30\n",
      "1224/1224 [==============================] - 97s 80ms/step - loss: 0.0551 - acc: 0.9826 - val_loss: 0.0085 - val_acc: 0.9974\n",
      "Epoch 26/30\n",
      "1224/1224 [==============================] - 97s 79ms/step - loss: 0.0538 - acc: 0.9829 - val_loss: 0.0065 - val_acc: 0.9980\n",
      "Epoch 27/30\n",
      "1224/1224 [==============================] - 97s 80ms/step - loss: 0.0519 - acc: 0.9835 - val_loss: 0.0081 - val_acc: 0.9974\n",
      "Epoch 28/30\n",
      "1224/1224 [==============================] - 98s 80ms/step - loss: 0.0532 - acc: 0.9828 - val_loss: 0.0087 - val_acc: 0.9970\n",
      "Epoch 29/30\n",
      "1224/1224 [==============================] - 98s 80ms/step - loss: 0.0530 - acc: 0.9833 - val_loss: 0.0103 - val_acc: 0.9964\n",
      "Epoch 30/30\n",
      "1224/1224 [==============================] - 98s 80ms/step - loss: 0.0516 - acc: 0.9835 - val_loss: 0.0117 - val_acc: 0.9960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26102fab160>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "start_time = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "\n",
    "model.fit_generator(train_generator, epochs=30, validation_data=val_generator,\n",
    "    steps_per_epoch=train_generator.__len__(),\n",
    "    validation_steps=val_generator.__len__(),\n",
    "    callbacks=[\n",
    "        TensorBoard(log_dir='./logs/%s' % (start_time)),\n",
    "        ModelCheckpoint('./models/%s.h5' % (start_time), monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n",
    "        ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, verbose=1, mode='auto')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "30c4c68e78bb9d0b80cdf9d7d3b2282ce2fa7ef3"
   },
   "outputs": [],
   "source": [
    "model.save(\"D:/PROJECTS/Github Clones/Converse-now/converse-now-v1.h5\")\n",
    "model.save_weights(\"D:/PROJECTS/Github Clones/Converse-now/conv-now-v1-weights.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envML",
   "language": "python",
   "name": "envml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
